{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12698eae-fee7-4e6d-a34e-5e0b7403de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torchvision.datasets.utils as dataset_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51649012-cd82-4485-8cc4-dc30e820f040",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 33\u001b[0m\n\u001b[1;32m     27\u001b[0m         arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(arr, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8) \n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mColoredMNIST\u001b[39;00m(\u001b[43mdatasets\u001b[49m\u001b[38;5;241m.\u001b[39mVisionDataset):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, env\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain1\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, target_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28msuper\u001b[39m(ColoredMNIST, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform,\n\u001b[1;32m     37\u001b[0m                                            target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "def color_grayscale_arr(arr, forground_color, background_color):\n",
    "    \"\"\"Converts grayscale image\"\"\"\n",
    "    assert arr.ndim == 2\n",
    "    dtype = arr.dtype\n",
    "    h, w = arr.shape\n",
    "    arr = np.reshape(arr, [h, w, 1])#增加一个“通道”维度\n",
    "    if background_color == \"black\":\n",
    "        if forground_color == \"red\":\n",
    "            arr = np.concatenate([arr,\n",
    "                              np.zeros((h, w, 2), dtype=dtype)], axis=2)#创建全零数组作为绿色和蓝色通道，表示全红色\n",
    "        elif forground_color == \"green\":\n",
    "            arr = np.concatenate([np.zeros((h, w, 1), dtype=dtype),\n",
    "                              arr,\n",
    "                              np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
    "        elif forground_color == \"white\":\n",
    "            arr = np.concatenate([arr, arr, arr], axis=2)\n",
    "    else:\n",
    "        if forground_color == \"yellow\":\n",
    "            arr = np.concatenate([arr, arr, np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
    "        else:\n",
    "            arr = np.concatenate([np.zeros((h, w, 2), dtype=dtype), arr], axis=2)\n",
    "\n",
    "        c = [255, 255, 255]\n",
    "        arr[:, :, 0] = (255 - arr[:, :, 0]) / 255 * c[0]\n",
    "        arr[:, :, 1] = (255 - arr[:, :, 1]) / 255 * c[1]\n",
    "        arr[:, :, 2] = (255 - arr[:, :, 2]) / 255 * c[2]\n",
    "        arr = np.clip(arr, 0, 255).astype(np.uint8) \n",
    "\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "class ColoredMNIST(datasets.VisionDataset):\n",
    "\n",
    "    def __init__(self, root='./data', env='train1', transform=None, target_transform=None):\n",
    "        super(ColoredMNIST, self).__init__(root, transform=transform,\n",
    "                                           target_transform=target_transform)\n",
    "\n",
    "        self.prepare_colored_mnist()\n",
    "        if env in ['train1', 'train2', 'train3', 'test1', 'test2']:\n",
    "            self.data_label_tuples = torch.load(os.path.join(self.root, 'ColoredMNIST', env) + '.pt',\n",
    "                                               weights_only=False)\n",
    "        elif env == 'all_train':\n",
    "            train1_data = torch.load(os.path.join(self.root, 'ColoredMNIST', 'train1.pt'),\n",
    "                                                weights_only=False ) \n",
    "            train2_data=torch.load(os.path.join(self.root, 'ColoredMNIST', 'train2.pt'),\n",
    "                                                weights_only=False)\n",
    "            train3_data=torch.load(os.path.join(self.root, 'ColoredMNIST', 'train3.pt'),\n",
    "                                                weights_only=False)\n",
    "            self.data_label_tuples = train1_data + train2_data + train3_data\n",
    "        else:\n",
    "            raise RuntimeError(f'{env} env unknown. Valid envs are train1, train2, train3, test1, test2, and all_train')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "    Args:\n",
    "        index (int): Index\n",
    "\n",
    "    Returns:\n",
    "        tuple: (image, target) where target is index of the target class.\n",
    "    \"\"\"\n",
    "        img, target = self.data_label_tuples[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_label_tuples)\n",
    "\n",
    "    def prepare_colored_mnist(self):\n",
    "        colored_mnist_dir = os.path.join(self.root, 'ColoredMNIST')\n",
    "        if os.path.exists(os.path.join(colored_mnist_dir, 'train1.pt')) \\\n",
    "                and os.path.exists(os.path.join(colored_mnist_dir, 'train2.pt')) \\\n",
    "                and os.path.exists(os.path.join(colored_mnist_dir, 'train3.pt')) \\\n",
    "                and os.path.exists(os.path.join(colored_mnist_dir, 'test1.pt')) \\\n",
    "                and os.path.exists(os.path.join(colored_mnist_dir, 'test2.pt')):\n",
    "            print('Colored MNIST dataset already exists')\n",
    "            return\n",
    "\n",
    "        print('Preparing Colored MNIST')\n",
    "        train_mnist = datasets.mnist.MNIST(self.root, train=True, download=True)\n",
    "\n",
    "        train1_set = []\n",
    "        train2_set = []\n",
    "        train3_set = []\n",
    "        test1_set, test2_set = [], []\n",
    "        for idx, (im, label) in enumerate(train_mnist):\n",
    "            if idx % 10000 == 0:\n",
    "                print(f'Converting image {idx}/{len(train_mnist)}')\n",
    "            im_array = np.array(im)\n",
    "            \n",
    "            # Assign a binary label y to the image based on the digit\n",
    "            binary_label = 0 if label < 5 else 1\n",
    "\n",
    "            # Color the image according to its environment label\n",
    "\n",
    "            if idx < 10000:\n",
    "                colored_arr = color_grayscale_arr(im_array, forground_color = \"red\", background_color = \"black\")\n",
    "                train1_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "            elif idx < 20000:\n",
    "                colored_arr = color_grayscale_arr(im_array, forground_color = \"green\", background_color = \"black\")\n",
    "                train2_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "            elif idx < 30000:\n",
    "                colored_arr = color_grayscale_arr(im_array, forground_color = \"white\", background_color = \"black\")\n",
    "                train3_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "            elif idx < 45000:\n",
    "                colored_arr = color_grayscale_arr(im_array, forground_color = \"yellow\", background_color = \"black\")\n",
    "                test1_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "            else:\n",
    "                colored_arr = color_grayscale_arr(im_array, forground_color = \"blue\", background_color = \"black\")\n",
    "                test2_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "                \n",
    "            # Image.fromarray(colored_arr).save('./data/sample/{}.png'.format(idx))\n",
    "\n",
    "        if not os.path.exists(colored_mnist_dir):\n",
    "            os.makedirs(colored_mnist_dir)\n",
    "        torch.save(train1_set, os.path.join(colored_mnist_dir, 'train1.pt'))\n",
    "        torch.save(train2_set, os.path.join(colored_mnist_dir, 'train2.pt'))\n",
    "        torch.save(train3_set, os.path.join(colored_mnist_dir, 'train3.pt'))\n",
    "        torch.save(test1_set, os.path.join(colored_mnist_dir, 'test1.pt'))\n",
    "        torch.save(test2_set, os.path.join(colored_mnist_dir, 'test2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b39810-fce9-454c-b136-9e61639b7f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Preparing Colored MNIST\n",
      "Converting image 0/60000\n",
      "Converting image 10000/60000\n",
      "Converting image 20000/60000\n",
      "Converting image 30000/60000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 1), |u1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\EST1601\\est1601_lab_env_v2_cu124\\est1601_py\\python_env\\Lib\\site-packages\\PIL\\Image.py:3315\u001b[39m, in \u001b[36mfromarray\u001b[39m\u001b[34m(obj, mode)\u001b[39m\n\u001b[32m   3314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3315\u001b[39m     mode, rawmode = \u001b[43m_fromarray_typemap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtypekey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   3316\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyError\u001b[39m: ((1, 1, 1), '|u1')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      6\u001b[39m transform=transforms.Compose([\n\u001b[32m      7\u001b[39m     transforms.ToTensor(),\n\u001b[32m      8\u001b[39m     transforms.Normalize((\u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m), (\u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m))\n\u001b[32m      9\u001b[39m ])\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m train_dataset1=\u001b[43mColoredMNIST\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./data\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m train_dataset2= ColoredMNIST(root=\u001b[33m'\u001b[39m\u001b[33m./data\u001b[39m\u001b[33m'\u001b[39m, env=\u001b[33m'\u001b[39m\u001b[33mtrain2\u001b[39m\u001b[33m'\u001b[39m, transform=transform)\n\u001b[32m     14\u001b[39m train_dataset3= ColoredMNIST(root=\u001b[33m'\u001b[39m\u001b[33m./data\u001b[39m\u001b[33m'\u001b[39m, env=\u001b[33m'\u001b[39m\u001b[33mtrain3\u001b[39m\u001b[33m'\u001b[39m, transform=transform)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mColoredMNIST.__init__\u001b[39m\u001b[34m(self, root, env, transform, target_transform)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root=\u001b[33m'\u001b[39m\u001b[33m./data\u001b[39m\u001b[33m'\u001b[39m, env=\u001b[33m'\u001b[39m\u001b[33mtrain1\u001b[39m\u001b[33m'\u001b[39m, transform=\u001b[38;5;28;01mNone\u001b[39;00m, target_transform=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28msuper\u001b[39m(ColoredMNIST, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m(root, transform=transform,\n\u001b[32m     35\u001b[39m                                        target_transform=target_transform)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_colored_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mtrain1\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtrain2\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtrain3\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtest1\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtest2\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28mself\u001b[39m.data_label_tuples = torch.load(os.path.join(\u001b[38;5;28mself\u001b[39m.root, \u001b[33m'\u001b[39m\u001b[33mColoredMNIST\u001b[39m\u001b[33m'\u001b[39m, env) + \u001b[33m'\u001b[39m\u001b[33m.pt\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     40\u001b[39m                                            weights_only=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mColoredMNIST.prepare_colored_mnist\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m idx < \u001b[32m45000\u001b[39m:\n\u001b[32m    110\u001b[39m     colored_arr = color_grayscale_arr(im_array, forground_color = \u001b[33m\"\u001b[39m\u001b[33myellow\u001b[39m\u001b[33m\"\u001b[39m, background_color = \u001b[33m\"\u001b[39m\u001b[33mblack\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     test1_set.append((\u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolored_arr\u001b[49m\u001b[43m)\u001b[49m, binary_label))\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    113\u001b[39m     colored_arr = color_grayscale_arr(im_array, forground_color = \u001b[33m\"\u001b[39m\u001b[33mblue\u001b[39m\u001b[33m\"\u001b[39m, background_color = \u001b[33m\"\u001b[39m\u001b[33mblack\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\EST1601\\est1601_lab_env_v2_cu124\\est1601_py\\python_env\\Lib\\site-packages\\PIL\\Image.py:3319\u001b[39m, in \u001b[36mfromarray\u001b[39m\u001b[34m(obj, mode)\u001b[39m\n\u001b[32m   3317\u001b[39m         typekey_shape, typestr = typekey\n\u001b[32m   3318\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot handle this data type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypekey_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypestr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3319\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   3320\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3321\u001b[39m     rawmode = mode\n",
      "\u001b[31mTypeError\u001b[39m: Cannot handle this data type: (1, 1, 1), |u1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "#图像预处理流程\n",
    "#转换为 Tensor，以及归一化\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "print(\"Loading\")\n",
    "train_dataset1=ColoredMNIST(root='./data', env='train1', transform=transform)\n",
    "train_dataset2= ColoredMNIST(root='./data', env='train2', transform=transform)\n",
    "train_dataset3= ColoredMNIST(root='./data', env='train3', transform=transform)\n",
    "test_dataset1= ColoredMNIST(root='./data', env='test1', transform=transform)\n",
    "test_dataset2= ColoredMNIST(root='./data', env='test2', transform=transform)\n",
    "\n",
    "# 打印数据集大小，确认加载成功\n",
    "print(f\"Size of train_dataset_1: {len(train_dataset1)}\")\n",
    "print(f\"Size of test_dataset_2: {len(test_dataset2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3cd0ae5-32ad-415c-b315-4b5f6934e3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ColoredMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "041ca2e0-60f6-42aa-99f0-f82ee9c8ca4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training30000\n",
      "Test1 15000\n",
      "Test2 15000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,ConcatDataset\n",
    "#把所有训练集搞在一起\n",
    "all_train_dataset = ConcatDataset([train_dataset1, train_dataset2, train_dataset3])\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "#dataloader\n",
    "my_train_loader = DataLoader(dataset=all_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "my_test_loader1 = DataLoader(dataset=test_dataset1, batch_size=BATCH_SIZE, shuffle=False)\n",
    "my_test_loader2 = DataLoader(dataset=test_dataset2, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Total training{len(all_train_dataset)}\")\n",
    "print(f\"Test1 {len(test_dataset1)}\")\n",
    "print(f\"Test2 {len(test_dataset2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df347714-4cc9-4df0-a3bb-9feb0b22d1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "see=next(iter(my_train_loader))[0][0]\n",
    "print(see.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cc3c5f0-d8a9-41a5-9caf-f0bd7e6299a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型结构和前向传播\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(LeNet5, self).__init__()\n",
    "        #输入是 3x28x28，输出是类别数\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16 * 5 * 5, 120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 16 * 5 * 5) #展平操作\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e765b97-8fb6-4562-bc24-4ba7ca1ae7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "my_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {my_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e14bd97-7660-4010-a32e-c594ab075489",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model=LeNet5(num_classes=2).to(my_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49c9ab4f-8f3b-4125-9a07-6b13ed75f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#损失函数和优化器\n",
    "my_loss=nn.CrossEntropyLoss()\n",
    "my_optimizer=optim.Adam(my_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1adc1446-cf3e-45bc-b565-bddffd6c601f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'criterion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss/\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFinished\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmy_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmy_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmy_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmy_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, loss, optimizer, device)\u001b[39m\n\u001b[32m     10\u001b[39m labels = labels.to(device)\n\u001b[32m     11\u001b[39m outputs = model(images)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m loss = \u001b[43mcriterion\u001b[49m(outputs, labels)\n\u001b[32m     13\u001b[39m optimizer.zero_grad()\n\u001b[32m     14\u001b[39m loss.backward()\n",
      "\u001b[31mNameError\u001b[39m: name 'criterion' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5 # 训练5epoch\n",
    "def train(model,train_loader,loss,optimizer,device):\n",
    "    print(\"\\nStarting training\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        running_loss=0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # 将数据移动到指定设备\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "    print(\"Finished\")\n",
    "train(my_model,my_train_loader,my_loss,my_optimizer,my_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2c173840-a4eb-49f0-9fb3-17247ab9c4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy50.45 %\n",
      "Accuracy51.03 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51.026666666666664"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试\n",
    "def test_model(model,test_loader,device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)#取最大概率作为结果\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy{accuracy:.2f} %')\n",
    "    return accuracy\n",
    "\n",
    "test_model(my_model,my_test_loader1,my_device)\n",
    "test_model(my_model,my_test_loader2,my_device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EST1601",
   "language": "python",
   "name": "est1601"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
