{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12698eae-fee7-4e6d-a34e-5e0b7403de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torchvision.datasets.utils as dataset_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "51649012-cd82-4485-8cc4-dc30e820f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_grayscale_arr(arr, forground_color, background_color):\n",
    "    \"\"\"Converts grayscale image\"\"\"\n",
    "    assert arr.ndim == 2\n",
    "    dtype = arr.dtype\n",
    "    h, w = arr.shape\n",
    "    arr = np.reshape(arr, [h, w, 1])#增加一个“通道”维度\n",
    "    if background_color == \"black\":\n",
    "        if forground_color == \"red\":\n",
    "            arr = np.concatenate([arr,\n",
    "                              np.zeros((h, w, 2), dtype=dtype)], axis=2)#创建全零数组作为绿色和蓝色通道，表示全红色\n",
    "        elif forground_color == \"green\":\n",
    "            arr = np.concatenate([np.zeros((h, w, 1), dtype=dtype),\n",
    "                              arr,\n",
    "                              np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
    "        elif forground_color == \"white\":\n",
    "            arr = np.concatenate([arr, arr, arr], axis=2)\n",
    "    else:\n",
    "        if forground_color == \"yellow\":\n",
    "            arr = np.concatenate([arr, arr, np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
    "        else:\n",
    "            arr = np.concatenate([np.zeros((h, w, 2), dtype=dtype), arr], axis=2)\n",
    "\n",
    "        c = [255, 255, 255]\n",
    "        arr[:, :, 0] = (255 - arr[:, :, 0]) / 255 * c[0]\n",
    "        arr[:, :, 1] = (255 - arr[:, :, 1]) / 255 * c[1]\n",
    "        arr[:, :, 2] = (255 - arr[:, :, 2]) / 255 * c[2]\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "class ColoredMNIST(datasets.VisionDataset):\n",
    "\n",
    "    def __init__(self, root='./data', env='train1', transform=None, target_transform=None):\n",
    "        super(ColoredMNIST, self).__init__(root, transform=transform,\n",
    "                                           target_transform=target_transform)\n",
    "\n",
    "        self.prepare_colored_mnist()\n",
    "        if env in ['train1', 'train2', 'train3', 'test1', 'test2']:\n",
    "            self.data_label_tuples = torch.load(os.path.join(self.root, 'ColoredMNIST', env) + '.pt',\n",
    "                                               weights_only=False)\n",
    "        elif env == 'all_train':\n",
    "            train1_data = torch.load(os.path.join(self.root, 'ColoredMNIST', 'train1.pt'),\n",
    "                                                weights_only=False ) \n",
    "            train2_data=torch.load(os.path.join(self.root, 'ColoredMNIST', 'train2.pt'),\n",
    "                                                weights_only=False)\n",
    "            train3_data=torch.load(os.path.join(self.root, 'ColoredMNIST', 'train3.pt'),\n",
    "                                                weights_only=False)\n",
    "            self.data_label_tuples = train1_data + train2_data + train3_data\n",
    "        else:\n",
    "            raise RuntimeError(f'{env} env unknown. Valid envs are train1, train2, train3, test1, test2, and all_train')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "    Args:\n",
    "        index (int): Index\n",
    "\n",
    "    Returns:\n",
    "        tuple: (image, target) where target is index of the target class.\n",
    "    \"\"\"\n",
    "        img, target = self.data_label_tuples[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_label_tuples)\n",
    "\n",
    "    def prepare_colored_mnist(self):\n",
    "        colored_mnist_dir = os.path.join(self.root, 'ColoredMNIST')\n",
    "        if os.path.exists(os.path.join(colored_mnist_dir, 'train1.pt')) \\\n",
    "                and os.path.exists(os.path.join(colored_mnist_dir, 'train2.pt')) \\\n",
    "                and os.path.exists(os.path.join(colored_mnist_dir, 'train3.pt')) \\\n",
    "                and os.path.exists(os.path.join(colored_mnist_dir, 'test1.pt')) \\\n",
    "                and os.path.exists(os.path.join(colored_mnist_dir, 'test2.pt')):\n",
    "            print('Colored MNIST dataset already exists')\n",
    "            return\n",
    "\n",
    "        print('Preparing Colored MNIST')\n",
    "        train_mnist = datasets.mnist.MNIST(self.root, train=True, download=True)\n",
    "\n",
    "        train1_set = []\n",
    "        train2_set = []\n",
    "        train3_set = []\n",
    "        test1_set, test2_set = [], []\n",
    "        for idx, (im, label) in enumerate(train_mnist):\n",
    "            if idx % 10000 == 0:\n",
    "                print(f'Converting image {idx}/{len(train_mnist)}')\n",
    "            im_array = np.array(im)\n",
    "            \n",
    "            # Assign a binary label y to the image based on the digit\n",
    "            binary_label = 0 if label < 5 else 1\n",
    "\n",
    "            # Color the image according to its environment label\n",
    "\n",
    "            if idx < 10000:\n",
    "                colored_arr = color_grayscale_arr(im_array, forground_color = \"red\", background_color = \"black\")\n",
    "                train1_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "            elif idx < 20000:\n",
    "                colored_arr = color_grayscale_arr(im_array, forground_color = \"green\", background_color = \"black\")\n",
    "                train2_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "            elif idx < 30000:\n",
    "                colored_arr = color_grayscale_arr(im_array, forground_color = \"white\", background_color = \"black\")\n",
    "                train3_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "            elif idx < 45000:\n",
    "                colored_arr = color_grayscale_arr(im_array, forground_color = \"yellow\", background_color = \"white\")\n",
    "                test1_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "            else:\n",
    "                colored_arr = color_grayscale_arr(im_array, forground_color = \"blue\", background_color = \"white\")\n",
    "                test2_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "                \n",
    "            # Image.fromarray(colored_arr).save('./data/sample/{}.png'.format(idx))\n",
    "\n",
    "        if not os.path.exists(colored_mnist_dir):\n",
    "            os.makedirs(colored_mnist_dir)\n",
    "        torch.save(train1_set, os.path.join(colored_mnist_dir, 'train1.pt'))\n",
    "        torch.save(train2_set, os.path.join(colored_mnist_dir, 'train2.pt'))\n",
    "        torch.save(train3_set, os.path.join(colored_mnist_dir, 'train3.pt'))\n",
    "        torch.save(test1_set, os.path.join(colored_mnist_dir, 'test1.pt'))\n",
    "        torch.save(test2_set, os.path.join(colored_mnist_dir, 'test2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e3b39810-fce9-454c-b136-9e61639b7f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Preparing Colored MNIST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9.91M/9.91M [05:25<00:00, 30.4kB/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 91.1kB/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1.65M/1.65M [00:27<00:00, 61.0kB/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 4.54k/4.54k [00:00<00:00, 940kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting image 0/60000\n",
      "Converting image 10000/60000\n",
      "Converting image 20000/60000\n",
      "Converting image 30000/60000\n",
      "Converting image 40000/60000\n",
      "Converting image 50000/60000\n",
      "Colored MNIST dataset already exists\n",
      "Colored MNIST dataset already exists\n",
      "Colored MNIST dataset already exists\n",
      "Colored MNIST dataset already exists\n",
      "Size of train_dataset_1: 10000\n",
      "Size of test_dataset_2: 15000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "#图像预处理流程\n",
    "#转换为 Tensor，以及归一化\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "print(\"Loading\")\n",
    "train_dataset1=ColoredMNIST(root='./data', env='train1', transform=transform)\n",
    "train_dataset2= ColoredMNIST(root='./data', env='train2', transform=transform)\n",
    "train_dataset3= ColoredMNIST(root='./data', env='train3', transform=transform)\n",
    "test_dataset1= ColoredMNIST(root='./data', env='test1', transform=transform)\n",
    "test_dataset2= ColoredMNIST(root='./data', env='test2', transform=transform)\n",
    "\n",
    "# 打印数据集大小，确认加载成功\n",
    "print(f\"Size of train_dataset_1: {len(train_dataset1)}\")\n",
    "print(f\"Size of test_dataset_2: {len(test_dataset2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f3cd0ae5-32ad-415c-b315-4b5f6934e3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ColoredMNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "041ca2e0-60f6-42aa-99f0-f82ee9c8ca4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training30000\n",
      "Test1 15000\n",
      "Test2 15000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,ConcatDataset\n",
    "#把所有训练集搞在一起\n",
    "all_train_dataset = ConcatDataset([train_dataset1, train_dataset2, train_dataset3])\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "#dataloader\n",
    "my_train_loader1 = DataLoader(dataset=train_dataset1, batch_size=BATCH_SIZE, shuffle=True)\n",
    "my_train_loader2 = DataLoader(dataset=train_dataset2, batch_size=BATCH_SIZE, shuffle=True)\n",
    "my_train_loader3 = DataLoader(dataset=train_dataset3, batch_size=BATCH_SIZE, shuffle=True)\n",
    "my_test_loader1 = DataLoader(dataset=train_dataset1, batch_size=BATCH_SIZE, shuffle=False)\n",
    "my_test_loader2 = DataLoader(dataset=test_dataset2, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Total training{len(all_train_dataset)}\")\n",
    "print(f\"Test1 {len(test_dataset1)}\")\n",
    "print(f\"Test2 {len(test_dataset2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df347714-4cc9-4df0-a3bb-9feb0b22d1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "see=next(iter(my_train_loader))[0][0]\n",
    "print(see.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4cc3c5f0-d8a9-41a5-9caf-f0bd7e6299a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型结构和前向传播\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(LeNet5, self).__init__()\n",
    "        #输入是 3x28x28，输出是类别数\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16 * 5 * 5, 120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 16 * 5 * 5) #展平操作\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e765b97-8fb6-4562-bc24-4ba7ca1ae7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "my_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {my_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5e14bd97-7660-4010-a32e-c594ab075489",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model=LeNet5(num_classes=2).to(my_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "49c9ab4f-8f3b-4125-9a07-6b13ed75f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#损失函数和优化器\n",
    "my_loss=nn.CrossEntropyLoss()\n",
    "my_optimizer=optim.Adam(my_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1adc1446-cf3e-45bc-b565-bddffd6c601f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting IRM training\n",
      "Epoch [1/10], Train Loss: 2.0835, Train Acc: 50.67%, Penalty: 0.0002, Penalty Weight: 0.00\n",
      "Epoch [2/10], Train Loss: 2.0605, Train Acc: 53.11%, Penalty: 0.0006, Penalty Weight: 0.03\n",
      "Epoch [3/10], Train Loss: 1.4472, Train Acc: 78.38%, Penalty: 0.0093, Penalty Weight: 0.08\n",
      "Epoch [4/10], Train Loss: 1.0128, Train Acc: 86.73%, Penalty: 0.0103, Penalty Weight: 0.15\n",
      "Epoch [5/10], Train Loss: 0.7520, Train Acc: 90.61%, Penalty: 0.0095, Penalty Weight: 0.23\n",
      "Epoch [6/10], Train Loss: 0.6090, Train Acc: 92.74%, Penalty: 0.0076, Penalty Weight: 0.33\n",
      "Epoch [7/10], Train Loss: 0.5011, Train Acc: 94.08%, Penalty: 0.0067, Penalty Weight: 0.44\n",
      "Epoch [8/10], Train Loss: 0.4507, Train Acc: 94.64%, Penalty: 0.0059, Penalty Weight: 0.57\n",
      "Epoch [9/10], Train Loss: 0.4061, Train Acc: 95.32%, Penalty: 0.0049, Penalty Weight: 0.70\n",
      "Epoch [10/10], Train Loss: 0.3710, Train Acc: 95.77%, Penalty: 0.0053, Penalty Weight: 0.84\n",
      "Finished IRM training\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 15 # 训练10epoch\n",
    "def irm_train(model, train_loaders, loss_fn, optimizer, device, num_epochs=NUM_EPOCHS, penalty_weight=1.0):\n",
    "    print(\"\\nStarting IRM training\")\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'penalty': []\n",
    "    }\n",
    "    # 创建用于IRM惩罚的可训练标量参数\n",
    "    dummy_w = torch.nn.Parameter(torch.tensor([1.0], device=device))\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_train = correct_train = 0\n",
    "        total_penalty = 0.0\n",
    "        # 动态调整惩罚权重(可选)\n",
    "        current_penalty_weight = penalty_weight * (epoch ** 1.6 / num_epochs ** 1.6)\n",
    "        # 获取所有环境的迭代器\n",
    "        iterators = [iter(loader) for loader in train_loaders]\n",
    "        while True:\n",
    "            try:\n",
    "                optimizer.zero_grad()\n",
    "                batch_erm_loss = 0.0\n",
    "                batch_penalty = 0.0\n",
    "                # 处理每个环境的数据\n",
    "                try:\n",
    "                    for env_iter in iterators:\n",
    "                        images, labels = next(env_iter)\n",
    "                    \n",
    "                        images = images.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                    # 前向传播\n",
    "                        outputs = model(images)\n",
    "                    # 计算ERM损失\n",
    "                        logits = outputs * dummy_w\n",
    "                        env_loss = loss_fn(logits, labels)\n",
    "                        batch_erm_loss += env_loss.mean()\n",
    "                    # 计算IRM惩罚项\n",
    "                        batch_penalty += compute_irm_penalty(env_loss, dummy_w)\n",
    "                except StopIteration:\n",
    "                    # 某个环境数据已遍历完\n",
    "                    break\n",
    "                # 计算总损失\n",
    "                total_loss = batch_erm_loss + current_penalty_weight * batch_penalty\n",
    "                # 反向传播和优化\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                # 统计信息\n",
    "                running_loss += batch_erm_loss.item()\n",
    "                total_penalty += batch_penalty.item()\n",
    "                # 计算准确率(使用最后一个环境的输出)\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "            except StopIteration:\n",
    "                break\n",
    "        \n",
    "        # 计算epoch统计量\n",
    "        avg_train_loss = running_loss / len(train_loaders[0])\n",
    "        avg_penalty = total_penalty / len(train_loaders[0])\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        \n",
    "        # 记录历史\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['penalty'].append(avg_penalty)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Train Acc: {train_acc:.2f}%, \"\n",
    "              f\"Penalty: {avg_penalty:.4f}, \"\n",
    "              f\"Penalty Weight: {current_penalty_weight:.2f}\")\n",
    "    print(\"Finished IRM training\")\n",
    "    return history\n",
    "def compute_irm_penalty(loss, dummy_w):\n",
    "    \"\"\"\n",
    "    计算IRM惩罚项\n",
    "    \n",
    "    参数:\n",
    "        loss: 每个样本的损失值(形状: [batch_size])\n",
    "        dummy_w: 可训练标量参数\n",
    "    \"\"\"\n",
    "    # 计算梯度∂loss/∂dummy_w\n",
    "    grad = torch.autograd.grad(loss.mean(), [dummy_w], create_graph=True)[0]\n",
    "    \n",
    "    # 惩罚项是梯度的平方和\n",
    "    penalty = torch.sum(grad ** 2)\n",
    "    return penalty\n",
    "my_history=irm_train(my_model,[my_train_loader1,my_train_loader2,my_train_loader3],my_loss,my_optimizer,my_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2c173840-a4eb-49f0-9fb3-17247ab9c4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy94.88 %\n",
      "Accuracy51.03 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51.026666666666664"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试\n",
    "def test_model(model,test_loader,device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)#取最大概率作为结果\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy{accuracy:.2f} %')\n",
    "    return accuracy\n",
    "\n",
    "test_model(my_model,my_test_loader1,my_device)\n",
    "test_model(my_model,my_test_loader2,my_device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
