{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e888680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: {}\".format(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eecab250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTReader(datasets.VisionDataset):\n",
    "    def __init__(self, root: str) -> None:\n",
    "        super().__init__(root)\n",
    "        self.data_label = torch.load(root)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, target = self.data_label[index]\n",
    "        return self.transform(image), target\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5, 1, 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 5, 1, 0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 5 * 5, 120),  \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(84, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84383585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_model(train_path, epochs=10, batch_size=32, learning_rate=0.01):\n",
    "    print(f\"Loading training data from: {train_path}\")\n",
    "    train_dataset = MNISTReader(train_path)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = LeNet().to(device)\n",
    "    loss_function = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            total_correct += (outputs.argmax(1) == targets).sum().item()\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = total_correct / total_samples\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b608abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_path, batch_size=1000):\n",
    "    print(f\"Testing on dataset: {test_path}\")\n",
    "    test_dataset = MNISTReader(test_path)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            total_correct += (outputs.argmax(1) == targets).sum().item()\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8bbf511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Training model on train1.pt\n",
      "Loading training data from: ./data/ColoredMNIST/train1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1055/2010497841.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data_label = torch.load(root)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.2430, Accuracy: 0.8876\n",
      "Epoch 2/10 - Loss: 0.0934, Accuracy: 0.9689\n",
      "Epoch 3/10 - Loss: 0.0688, Accuracy: 0.9780\n",
      "Epoch 4/10 - Loss: 0.0569, Accuracy: 0.9800\n",
      "Epoch 5/10 - Loss: 0.0471, Accuracy: 0.9845\n",
      "Epoch 6/10 - Loss: 0.0518, Accuracy: 0.9828\n",
      "Epoch 7/10 - Loss: 0.0343, Accuracy: 0.9863\n",
      "Epoch 8/10 - Loss: 0.0300, Accuracy: 0.9896\n",
      "Epoch 9/10 - Loss: 0.0361, Accuracy: 0.9870\n",
      "Epoch 10/10 - Loss: 0.0391, Accuracy: 0.9888\n",
      "Testing on dataset: ./data/ColoredMNIST/test1.pt\n",
      "Test accuracy: 0.5875\n",
      "Testing on dataset: ./data/ColoredMNIST/test2.pt\n",
      "Test accuracy: 0.9741\n",
      "==================================================\n",
      "Training model on train2.pt\n",
      "Loading training data from: ./data/ColoredMNIST/train2.pt\n",
      "Epoch 1/10 - Loss: 0.2030, Accuracy: 0.9145\n",
      "Epoch 2/10 - Loss: 0.0871, Accuracy: 0.9709\n",
      "Epoch 3/10 - Loss: 0.0568, Accuracy: 0.9804\n",
      "Epoch 4/10 - Loss: 0.0406, Accuracy: 0.9869\n",
      "Epoch 5/10 - Loss: 0.0457, Accuracy: 0.9858\n",
      "Epoch 6/10 - Loss: 0.0418, Accuracy: 0.9871\n",
      "Epoch 7/10 - Loss: 0.0278, Accuracy: 0.9916\n",
      "Epoch 8/10 - Loss: 0.0351, Accuracy: 0.9878\n",
      "Epoch 9/10 - Loss: 0.0439, Accuracy: 0.9866\n",
      "Epoch 10/10 - Loss: 0.0230, Accuracy: 0.9922\n",
      "Testing on dataset: ./data/ColoredMNIST/test1.pt\n",
      "Test accuracy: 0.9839\n",
      "Testing on dataset: ./data/ColoredMNIST/test2.pt\n",
      "Test accuracy: 0.5911\n",
      "==================================================\n",
      "Training model on train3.pt\n",
      "Loading training data from: ./data/ColoredMNIST/train3.pt\n",
      "Epoch 1/10 - Loss: 0.2515, Accuracy: 0.8896\n",
      "Epoch 2/10 - Loss: 0.1070, Accuracy: 0.9624\n",
      "Epoch 3/10 - Loss: 0.0821, Accuracy: 0.9731\n",
      "Epoch 4/10 - Loss: 0.0712, Accuracy: 0.9753\n",
      "Epoch 5/10 - Loss: 0.0627, Accuracy: 0.9797\n",
      "Epoch 6/10 - Loss: 0.0611, Accuracy: 0.9801\n",
      "Epoch 7/10 - Loss: 0.0600, Accuracy: 0.9802\n",
      "Epoch 8/10 - Loss: 0.0481, Accuracy: 0.9851\n",
      "Epoch 9/10 - Loss: 0.0417, Accuracy: 0.9840\n",
      "Epoch 10/10 - Loss: 0.0422, Accuracy: 0.9866\n",
      "Testing on dataset: ./data/ColoredMNIST/test1.pt\n",
      "Test accuracy: 0.9715\n",
      "Testing on dataset: ./data/ColoredMNIST/test2.pt\n",
      "Test accuracy: 0.9745\n",
      "\n",
      "All results:\n",
      "Model trained on train1.pt:\n",
      "  Test on test1.pt: Accuracy = 0.5875\n",
      "  Test on test2.pt: Accuracy = 0.9741\n",
      "Model trained on train2.pt:\n",
      "  Test on test1.pt: Accuracy = 0.9839\n",
      "  Test on test2.pt: Accuracy = 0.5911\n",
      "Model trained on train3.pt:\n",
      "  Test on test1.pt: Accuracy = 0.9715\n",
      "  Test on test2.pt: Accuracy = 0.9745\n"
     ]
    }
   ],
   "source": [
    "train_files = [\n",
    "    './data/ColoredMNIST/train1.pt',\n",
    "    './data/ColoredMNIST/train2.pt',\n",
    "    './data/ColoredMNIST/train3.pt',\n",
    "]\n",
    "# 测试集文件列表\n",
    "test_files = [\n",
    "    './data/ColoredMNIST/test1.pt',\n",
    "    './data/ColoredMNIST/test2.pt',\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for train_path in train_files:\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Training model on {os.path.basename(train_path)}\")\n",
    "    model = train_one_model(train_path, epochs=10)\n",
    "\n",
    "    # 测试模型在两个测试集上的表现\n",
    "    accuracies = {}\n",
    "    for test_path in test_files:\n",
    "        acc = test_model(model, test_path)\n",
    "        accuracies[os.path.basename(test_path)] = acc\n",
    "    results[os.path.basename(train_path)] = accuracies\n",
    "\n",
    "print(\"\\nAll results:\")\n",
    "for train_set, test_accs in results.items():\n",
    "    print(f\"Model trained on {train_set}:\")\n",
    "    for test_set, acc in test_accs.items():\n",
    "        print(f\"  Test on {test_set}: Accuracy = {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
