{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "629d6ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torchvision.datasets.utils as dataset_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc1bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_grayscale_arr(arr, forground_color, background_color):\n",
    "    \"\"\"Converts grayscale image\"\"\"\n",
    "    assert arr.ndim == 2\n",
    "    dtype = arr.dtype\n",
    "    h, w = arr.shape\n",
    "    arr = np.reshape(arr, [h, w, 1])\n",
    "    if background_color == \"black\":\n",
    "        if forground_color == \"red\":\n",
    "            arr = np.concatenate([arr,\n",
    "                              np.zeros((h, w, 2), dtype=dtype)], axis=2)\n",
    "        elif forground_color == \"green\":\n",
    "            arr = np.concatenate([np.zeros((h, w, 1), dtype=dtype),\n",
    "                              arr,\n",
    "                              np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
    "        elif forground_color == \"white\":\n",
    "            arr = np.concatenate([arr, arr, arr], axis=2)\n",
    "    else:\n",
    "        if forground_color == \"yellow\":\n",
    "            arr = np.concatenate([arr, arr, np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
    "        else:\n",
    "            arr = np.concatenate([np.zeros((h, w, 2), dtype=dtype), arr], axis=2)\n",
    "\n",
    "        c = [255, 255, 255]\n",
    "        arr[:, :, 0] = (255 - arr[:, :, 0]) / 255 * c[0]\n",
    "        arr[:, :, 1] = (255 - arr[:, :, 1]) / 255 * c[1]\n",
    "        arr[:, :, 2] = (255 - arr[:, :, 2]) / 255 * c[2]\n",
    "    arr = arr.astype(np.uint8) \n",
    "\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "class ColoredMNIST(datasets.VisionDataset):\n",
    "\n",
    "    def __init__(self, root='./data', env='train1', transform=None, target_transform=None):\n",
    "        super(ColoredMNIST, self).__init__(root, transform=transform,\n",
    "                                           target_transform=target_transform)\n",
    "\n",
    "        self.prepare_colored_mnist()\n",
    "        if env in ['train1', 'train2', 'train3', 'test1', 'test2']:\n",
    "            self.data_label_tuples = torch.load(os.path.join(self.root, 'ColoredMNIST', env) + '.pt')\n",
    "        elif env == 'all_train':\n",
    "            self.data_label_tuples = torch.load(os.path.join(self.root, 'ColoredMNIST', 'train1.pt')) + \\\n",
    "                                     torch.load(os.path.join(self.root, 'ColoredMNIST', 'train2.pt')) + \\\n",
    "                                     torch.load(os.path.join(self.root, 'ColoredMNIST', 'train3.pt'))\n",
    "        else:\n",
    "            raise RuntimeError(f'{env} env unknown. Valid envs are train1, train2, train3, test1, test2, and all_train')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "    Args:\n",
    "        index (int): Index\n",
    "\n",
    "    Returns:\n",
    "        tuple: (image, target) where target is index of the target class.\n",
    "    \"\"\"\n",
    "        img, target = self.data_label_tuples[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_label_tuples)\n",
    "\n",
    "    def prepare_colored_mnist(self):\n",
    "        colored_mnist_dir = os.path.join(self.root, 'ColoredMNIST')\n",
    "        if os.path.exists(os.path.join(colored_mnist_dir, 'train1.pt')) \\\n",
    "                and os.path.exists(os.path.join(colored_mnist_dir, 'train2.pt')) \\\n",
    "                and os.path.exists(os.path.join(colored_mnist_dir, 'train3.pt')) \\\n",
    "                and os.path.exists(os.path.join(colored_mnist_dir, 'test1.pt')) \\\n",
    "                and os.path.exists(os.path.join(colored_mnist_dir, 'test2.pt')):\n",
    "            print('Colored MNIST dataset already exists')\n",
    "            return\n",
    "\n",
    "        print('Preparing Colored MNIST')\n",
    "        train_mnist = datasets.mnist.MNIST(self.root, train=True, download=True)\n",
    "\n",
    "        train1_set = []\n",
    "        train2_set = []\n",
    "        train3_set = []\n",
    "        test1_set, test2_set = [], []\n",
    "        for idx, (im, label) in enumerate(train_mnist):\n",
    "            if idx % 10000 == 0:\n",
    "                print(f'Converting image {idx}/{len(train_mnist)}')\n",
    "            im_array = np.array(im)\n",
    "            \n",
    "            # Assign a binary label y to the image based on the digit\n",
    "            binary_label = 0 if label < 5 else 1\n",
    "\n",
    "            # Color the image according to its environment label\n",
    "\n",
    "            if idx < 10000:\n",
    "                colored_arr = color_grayscale_arr(im_array, forground_color = \"red\", background_color = \"black\")\n",
    "                train1_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "            elif idx < 20000:\n",
    "                colored_arr = color_grayscale_arr(im_array, forground_color = \"green\", background_color = \"black\")\n",
    "                train2_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "            elif idx < 30000:\n",
    "                colored_arr = color_grayscale_arr(im_array, forground_color = \"white\", background_color = \"black\")\n",
    "                train3_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "            elif idx < 45000:\n",
    "                colored_arr = color_grayscale_arr(im_array, forground_color = \"green\", background_color = \"black\")\n",
    "                test1_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "            else:\n",
    "                colored_arr = color_grayscale_arr(im_array, forground_color = \"red\", background_color = \"black\")\n",
    "                test2_set.append((Image.fromarray(colored_arr), binary_label))\n",
    "                \n",
    "            # Image.fromarray(colored_arr).save('./data/sample/{}.png'.format(idx))\n",
    "\n",
    "        if not os.path.exists(colored_mnist_dir):\n",
    "            os.makedirs(colored_mnist_dir)\n",
    "        torch.save(train1_set, os.path.join(colored_mnist_dir, 'train1.pt'))\n",
    "        torch.save(train2_set, os.path.join(colored_mnist_dir, 'train2.pt'))\n",
    "        torch.save(train3_set, os.path.join(colored_mnist_dir, 'train3.pt'))\n",
    "        torch.save(test1_set, os.path.join(colored_mnist_dir, 'test1.pt'))\n",
    "        torch.save(test2_set, os.path.join(colored_mnist_dir, 'test2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67aca898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Colored MNIST dataset already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2875/4197987548.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data_label_tuples = torch.load(os.path.join(self.root, 'ColoredMNIST', env) + '.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colored MNIST dataset already exists\n",
      "Colored MNIST dataset already exists\n",
      "Colored MNIST dataset already exists\n",
      "Colored MNIST dataset already exists\n"
     ]
    }
   ],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "print(\"Loading\")\n",
    "train_dataset1=ColoredMNIST(root='./data', env='train1', transform=transform)\n",
    "train_dataset2= ColoredMNIST(root='./data', env='train2', transform=transform)\n",
    "train_dataset3= ColoredMNIST(root='./data', env='train3', transform=transform)\n",
    "test_dataset1= ColoredMNIST(root='./data', env='test1', transform=transform)\n",
    "test_dataset2= ColoredMNIST(root='./data', env='test2', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7dfc498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24370/1836822511.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load('./data/ColoredMNIST/test2.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in train1: 15000\n",
      "Sample 0: label = 0, image size = (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGZCAYAAABmNy2oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAMu0lEQVR4nO3cW4zWdX7H8e8jAyxFSwRc5RAL4qoFJF6IrXqjshcstZoaIjVNTNCmGuqhRoiHeOCYJrUXGhKCJghGbKQm1NMaz8Y0LU2g6aZCtJFEqjQdjEi2rhUQ+vRi00+WAgo/R2YGX6+EC4fnM8/PC3zzmxn/nW632y0AqKpT+vsAAAwcogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiwICwbt266nQ6tWXLlj75fJ1Op2677bY++Vy/+TkXL17cvP/6669ryZIlNWnSpBo+fHhdcMEFtXLlyr47IPSBnv4+APxQLFiwoJ5++ulatmxZzZw5s1577bW6884764svvqj777+/v48HVSUKcEJs27at1qxZUytWrKhFixZVVdUVV1xRu3fvruXLl9ett95ao0eP7udTgi8fMYjs3bu37r777rroootq1KhRNXr06Lr00kvrhRdeOOrm8ccfr/POO6+GDx9eU6dOrWefffaw1/T29tYtt9xSEydOrGHDhtXkyZNryZIldeDAgT47+/PPP1/dbrfmz59/yMfnz59fX331Vb366qt99l7wXbgpMGjs27evPv/881q4cGFNmDCh9u/fX2+++WZdd911tXbt2rrxxhsPef2LL75Y77zzTi1durRGjhxZq1atqhtuuKF6enpq7ty5VfXrIFxyySV1yimn1EMPPVRTpkypTZs21fLly2vHjh21du3abzzTpEmTqqpqx44d3/i6rVu31hlnnFFnnXXWIR+fMWNGfh8GAlFg0Bg1atQh/5E+ePBgzZo1q/bs2VOPPvroYVH47LPPavPmzXXmmWdWVdWcOXNq+vTpdd999yUKixcvrj179tS2bdvq7LPPrqqqWbNm1YgRI2rhwoW1aNGimjp16lHP1NNzbH+Edu/efcQvD40cObKGDRtWu3fvPqbPA983Xz5iUHnuuefq8ssvr1NPPbV6enpq6NChtWbNmnr//fcPe+2sWbMShKqqIUOG1Lx582r79u21c+fOqqp6+eWX68orr6zx48fXgQMH8utnP/tZVVW9++6733ie7du31/bt24/p7J1Op+n34EQSBQaNjRs31vXXX18TJkyo9evX16ZNm2rz5s1100031d69ew97/f//Us1vfuz//ma+a9eueumll2ro0KGH/Jo2bVpV/fq20RfGjBlzxNvAl19+Wfv37/dNZgYMXz5i0Fi/fn1Nnjy5NmzYcMjfrPft23fE1/f29h71Y2PGjKmqqrFjx9aMGTNqxYoVR/wc48eP/67HrqqqCy+8sJ599tnq7e09JFbvvfdeVVVNnz69T94Hvis3BQaNTqdTw4YNOyQIvb29R/3po7feeqt27dqVfz548GBt2LChpkyZUhMnTqyqqquvvrq2bt1aU6ZMqYsvvviwX30VhWuvvbY6nU499dRTh3x83bp1NWLEiJo9e3afvA98V24KDChvv/32EX+SZ86cOXX11VfXxo0ba8GCBTV37tz65JNPatmyZTVu3Lj68MMPD9uMHTu2rrrqqnrwwQfz00cffPDBIT+WunTp0nrjjTfqsssuqzvuuKPOP//82rt3b+3YsaNeeeWVWr16dQJyJOeee25V1bd+X2HatGl1880318MPP1xDhgypmTNn1uuvv15PPPFELV++3JePGDBEgQHlnnvuOeLHP/roo5o/f359+umntXr16nryySfrnHPOqXvvvbd27txZS5YsOWxzzTXX1LRp0+qBBx6ojz/+uKZMmVLPPPNMzZs3L68ZN25cbdmypZYtW1aPPPJI7dy5s0477bSaPHlyzZ49u04//fRvPO/x/L8Mq1atqgkTJtTKlSurt7e3Jk2aVI899ljdfvvtx/w54PvW6Xa73f4+BAADg+8pABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAERPfx+AH5L3G3crGzYbGzbnNmw6DZsrGjZVVX/asPmdxvfih8pNAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACA63W6329+HYDD6r4bN7zW+17817k424xo2bzZsfrdhw8nCTQGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgevr7AAxWGxs2J/LBdmc1bFoeBLe5YfOrhk1V1X82bO5q2LzasOFk4aYAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQHS63W63vw/BYPRxw2Z143u927D5u4bNjxs2S07QptVPGjYtT3797YYNA5GbAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEB4IB7EOw2b6xo2v2zYtLqmYfM3DZvfatgwELkpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQH4nGS+vuGzZyGzZcNm1ZXNmx+3rD5UcOGk4WbAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAED09PcB4Nutbdjc3bA5UQ+3+8PG3SMNGw+34/i4KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQnpJKo48aNi1PLq2qer5xdyK0/DutaHyvYY07OHZuCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgDR6Xa73f4+BP3tHxs2f9yw2dmwOZHGNGz+uWFzdsMGTgw3BQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYDo6e8DMBB83rAZ6A+3a7G7YfOThs39DZuqqpsbNhMb34sfKjcFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgOh0u91ufx+C/vbLhs3fNmz+smFTVfUfjbvjdbBh8z99foqj+3HD5pKGzdMNm1ENGwYiNwUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGA8EA8iH9q2LzQsFnbsKmq+rRxd7x+v2HzcsNmdMOG75ubAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgDhKalwwv1L4+62hs2mxvc6Xi81bP6gz0/Bd+emAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABA9/X0A+OH5UePuvT49BRyJmwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAeCAeg8D9DZutDZvnGzZfNmz+rGFTVfWrxt3x+qOGzVV9fgr6h5sCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQHggHo3+u2FzV+N7rW3YPNWw+deGzV80bP6hYVPV9sf1lobNXzVsRjRsGIjcFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQCi0+12u/19CAajf2/YTO7zUxzdnzRstjVsftGwafXThs3rfX4KTm5uCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgBET38fgMHq5/19gG/xzAl6n07D5q7G97q1cQfHzk0BgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIDwQj0aT+vsA34MZDZu/btj8tGEDJ4abAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEB0ut1ut78PwWC0r2Hzi8b3+vOGzRUNm0UNmzMbNjBwuSkAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhAfiARBuCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDE/wLv0gHn1jPOUAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: label = 0, image size = (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2: label = 0, image size = (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = torch.load('./data/ColoredMNIST/test2.pt')\n",
    "print(f'Total samples in train1: {len(data)}')\n",
    "\n",
    "# 查看前5个样本\n",
    "for i in range(3):\n",
    "    img, label = data[i]\n",
    "    print(f'Sample {i}: label = {label}, image size = {img.size}')\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.title(f'Label: {label}')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
